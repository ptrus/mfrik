from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout
from sklearn.datasets import load_boston
import numpy as np
from datetime import datetime
from sklearn.feature_selection import VarianceThreshold
from keras.regularizers import l2, activity_l2
from keras.layers.advanced_activations import PReLU
from keras.layers.normalization import BatchNormalization
from keras.callbacks import EarlyStopping
from math import sqrt
from utils import tsv_batches_generator, read_tsv_batch
from sklearn.preprocessing import StandardScaler
from keras.optimizers import SGD

if __name__ == '__main__':
    # base = "/home/peterus/Downloads/"
    # base = "C:\\Users\\Peter\\Downloads\\ccdm_large.tsv\\"
    base = "D:\\mfrik_data\\"
    base_output = "C:\\mfrik_data\\"
    # train = base+ file + "-preprocessed.tsv"
    train = base + 'cdm_all.tsv-preprocessed-ALL95.tsv'
    predict = base + 'ccdm_test.tsv-preprocessed-ALL95.tsv'

    number_of_samples = 0
    with open(train, 'r') as f:
        header = f.readline().strip().split('\t')
        for line in f:
            number_of_samples += 1

    print "done reading"


    model = Sequential()

    model.add(Dense(400, batch_input_shape=(None, len(header)-1)))
    model.add(PReLU())
    model.add(Dropout(0.5))
    model.add(BatchNormalization())

    model.add(Dense(200))
    model.add(PReLU())
    model.add(Dropout(0.5))
    model.add(BatchNormalization())

    model.add(Dense(100))
    model.add(PReLU())
    model.add(Dropout(0.5))
    model.add(BatchNormalization())

    model.add(Dense(1, activation='linear'))
    sgd = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)

    model.compile(loss='mse', optimizer=sgd)
    early_stopping = EarlyStopping(monitor='val_loss', patience=2)

    # fit the model on the batches generated by datagen.flow()

    generator = tsv_batches_generator(train, first_line=False, batchsize=1000)
    model.fit_generator(generator.train(),
                        samples_per_epoch=2375000,
                        nb_epoch=100,
                        validation_data=generator.valid(),
                        nb_val_samples=1000,
                        callbacks=[early_stopping])

    valid = generator.valid().next()
    score = model.evaluate(valid[0], valid[1])
    print sqrt(score)

    scaler = generator.standardscaler
    with open(base + 'predictions_keras.tsv', 'w') as fout:
        for batch in read_tsv_batch(predict, batchsize=1000):
            x = batch[:, 1:]
            x[x == 'null'] = '0'
            x = x.astype(float)
            x = scaler.transform(x)
            pred = model.predict(x)
            for y in pred:
                fout.write("{0:.3f}".format(y[0]) + '\n')

'''
model.train_on_batch(X, y)
model.test_on_batch(X, y)
model.fit_generator(data_generator, samples_per_epoch, nb_epoch).
'''