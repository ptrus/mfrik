train_data:
year: 2016
month: 4
days: 1 to 15

test_data:
year: 2016
month: 4
days: 1 to 15

FEATURES PREPROCESS:
- discetasize
- parse timestamps
- parse JSONS

todo:
- geoloc!!
- cluster based on target
    - find similarities within clusters
    - predict extreme values (after removing extreme extremes)
          - put a flag to extreme values
          - classification to predict the flag
          - predict the flag for test samples
        - TIMESTAMP SEEMS MOST IMPORTANT
  """
    (1) determine what revenue threshold you want to use as "revenue outlier", and add a Y/N flag for that in your training dataset.

    (2) use a classifier (or ensemble of classifiers) on the training dataset, with RevenueOutlier as the target. Could generate Y/N predictions, but I'm thinking that a probability value would be "safer", in that Y/N is either right or catastrophically wrong, whereas probabilities are more nuanced.

    (3) Add the predictions from Step 2 into the training dataset, as new field "OutlierProbability" .(If you as a human like meaningful names, machine learning algorithms don't care.)

    (4) Apply the Step 2 model to the test dataset to generate predictions, and add those values to the test data.

*** (5) Drop the original RevenueOutlier field out of the training dataset. Only use the (predicted) OutlierProbability. Ernest (and others) may have done this, but it's not explicitly stated in the posted comments. This is a somewhat counter-intuitive thing to do, why would you use the predicted value, which obviously will have some error fuzziness, over the actual value which you have, and which has zero error? And the reason is that you need an apples-to-apples match in what these things mean when the ML algorithms use them in train and then test; apples and pineapples isn't going to work very well. (But the extra error from using pineapples will be invisible, because there's no way to measure it, except "didn't improve anything on the LeaderBoard.")

    (6) Now train your model/s and generate predictions for the test dataset, as normal.
    """

    """
    1) Feature Engineering
        i) Square root transformation was applied to the obfuscated P variables with maximum value >= 10, to make them into the same scale, as well as the target variable “revenue”.


    i) Missing value indicator for multiple P variables, i.e. P14 to P18, P24 to P27, and P30 to P37 was created to help differentiate synthetic and real test data.


    i) Zeroes were treated as missing values and mice imputation was applied on training and test set separately.

"""

Dodaj: - povprecna vrednost od accounta
       - povprecna vrednost od CDNNAME
       - povprecna vrednost SDK
       - povprecna vrednost za timeframe
       - povprecna vrednost PLATFORM (PLATFORM_IOS - zgleda dobr)
       - povprecna vrednost CREATIVETYPE_Interscroller
       - povprecna vrednost EXTERNALADSERVER?
       - povprecna UA_DEVICETYPE?
       - number of nulls?

"""
    - log,sqrt scalings, anscombe transform
    - knn

-Feature 1: Distances to nearest neighbours of each classes
-Feature 5: Distances to nearest neighbours of each classed in T-SNE space (3 dimensions)
-Feature 6: Clustering features of original dataset!!
-

"""

'''

The main idea of my solution is stacking. Stacking helps you to combine different methods’ predictions of Y (or labels when it comes to multiclass problems) as “metafeatures”. Basically, to obtain metafeature for train, you split your data into K folds, training K models on K-1 parts while making prediction for 1 part that was left aside for each K-1 group. To obtain metafeature for test, you can average predictions from these K models or make single prediction based on all train data. After that you train metaclassifier on features & metafeatures and average predictions if you have several metaclassifiers.

In the beginning of working on the competition I found useful to split data in two groups : (1) train & test, (2) TF-IDF(train) & TF-IDF(test). Many parts of my solution use these two groups in parallel.

'''